{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1fbfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install sounddevice scipy openai-whisper bark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517da68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing voice assistant... This might take a minute...\n",
      "Initialization complete! Ready to chat!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:10000 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, BarkModel\n",
    "import whisper\n",
    "import sounddevice as sd\n",
    "import scipy.io.wavfile as wavfile\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class VoiceAssistant:\n",
    "    def __init__(self, whisper_model_path=\"small.pt\"):\n",
    "        print(\"Initializing voice assistant... This might take a minute...\")\n",
    "        \n",
    "        # Initialize Bark\n",
    "        self.bark_processor = AutoProcessor.from_pretrained(\"suno/bark\")\n",
    "        self.bark_model = BarkModel.from_pretrained(\"suno/bark\")\n",
    "        self.voice_preset = \"v2/en_speaker_6\"\n",
    "        \n",
    "        # Initialize local Whisper model\n",
    "        self.whisper_model = whisper.load_model(whisper_model_path)\n",
    "        \n",
    "        # Set device\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.bark_model.to(self.device)\n",
    "        \n",
    "        print(\"Initialization complete! Ready to chat!\")\n",
    "    \n",
    "    def speak(self, text):\n",
    "        \"\"\"Convert text to speech using Bark\"\"\"\n",
    "        inputs = self.bark_processor(text, voice_preset=self.voice_preset)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(self.device)\n",
    "            \n",
    "        audio_array = self.bark_model.generate(**inputs)\n",
    "        audio_array = audio_array.cpu().numpy().squeeze()\n",
    "        \n",
    "        # Normalize audio\n",
    "        audio_array = audio_array / np.max(np.abs(audio_array))\n",
    "        \n",
    "        # Play audio\n",
    "        sd.play(audio_array, samplerate=24000)\n",
    "        sd.wait()\n",
    "    \n",
    "    def listen(self, duration=5, sample_rate=16000):\n",
    "        \"\"\"Record audio and convert to text using local Whisper\"\"\"\n",
    "        print(\"Listening...\")\n",
    "        \n",
    "        # Record audio\n",
    "        recording = sd.rec(int(duration * sample_rate), \n",
    "                         samplerate=sample_rate, \n",
    "                         channels=1)\n",
    "        sd.wait()\n",
    "        \n",
    "        # Save recording temporarily\n",
    "        wavfile.write(\"temp_recording.wav\", sample_rate, recording)\n",
    "        \n",
    "        # Transcribe with local Whisper model\n",
    "        result = self.whisper_model.transcribe(\"temp_recording.wav\")\n",
    "        transcription = result[\"text\"]\n",
    "        \n",
    "        return transcription.strip().lower()\n",
    "    \n",
    "    def chat(self):\n",
    "        \"\"\"Main conversation loop\"\"\"\n",
    "        self.speak(\"Hello! I'm your voice assistant. How can I help you today?\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                # Listen to user input\n",
    "                user_input = self.listen()\n",
    "                print(f\"You said: {user_input}\")\n",
    "                \n",
    "                # Check for exit commands\n",
    "                if user_input in ['goodbye', 'bye', 'exit', 'quit']:\n",
    "                    self.speak(\"Goodbye! Have a great day!\")\n",
    "                    break\n",
    "                \n",
    "                # Generate response (you can make this more sophisticated)\n",
    "                response = f\"You said: {user_input}\"\n",
    "                print(f\"Assistant: {response}\")\n",
    "                self.speak(response)\n",
    "                \n",
    "                clear_output(wait=True)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {str(e)}\")\n",
    "                break\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize with path to your local Whisper model\n",
    "    model_path_whisper = os.path.expanduser(\"~/ml/models/whisper_models/small.pt\")\n",
    "    assistant = VoiceAssistant(whisper_model_path=model_path_whisper)\n",
    "    assistant.chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca06f99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing voice assistant... This might take a minute...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "/Users/hissain/ml/models/whisper_models does not appear to have a file named preprocessor_config.json. Checkout 'https://huggingface.co//Users/hissain/ml/models/whisper_models/tree/main' for available files.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 114\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Usage example\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 114\u001b[0m     assistant \u001b[38;5;241m=\u001b[39m \u001b[43mVoiceAssistant\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     assistant\u001b[38;5;241m.\u001b[39mchat()\n",
      "Cell \u001b[0;32mIn[2], line 24\u001b[0m, in \u001b[0;36mVoiceAssistant.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvoice_preset \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv2/en_speaker_6\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Initialize Whisper\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwhisper_processor \u001b[38;5;241m=\u001b[39m \u001b[43mWhisperProcessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dir_whisper\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwhisper_model \u001b[38;5;241m=\u001b[39m WhisperForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_dir_whisper)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Set device\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/transformers/processing_utils.py:465\u001b[0m, in \u001b[0;36mProcessorMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    463\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m token\n\u001b[0;32m--> 465\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_arguments_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m processor_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_processor_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_args_and_dict(args, processor_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/transformers/processing_utils.py:511\u001b[0m, in \u001b[0;36mProcessorMixin._get_arguments_from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    509\u001b[0m         attribute_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(transformers_module, class_name)\n\u001b[0;32m--> 511\u001b[0m     args\u001b[38;5;241m.\u001b[39mappend(\u001b[43mattribute_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m args\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/transformers/feature_extraction_utils.py:372\u001b[0m, in \u001b[0;36mFeatureExtractionMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    370\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m token\n\u001b[0;32m--> 372\u001b[0m feature_extractor_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_extractor_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_dict(feature_extractor_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/transformers/feature_extraction_utils.py:498\u001b[0m, in \u001b[0;36mFeatureExtractionMixin.get_feature_extractor_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    495\u001b[0m feature_extractor_file \u001b[38;5;241m=\u001b[39m FEATURE_EXTRACTOR_NAME\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 498\u001b[0m     resolved_feature_extractor_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_extractor_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/transformers/utils/hub.py:369\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(resolved_file):\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _raise_exceptions_for_missing_entries:\n\u001b[0;32m--> 369\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Checkout \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/tree/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m         )\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    374\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: /Users/hissain/ml/models/whisper_models does not appear to have a file named preprocessor_config.json. Checkout 'https://huggingface.co//Users/hissain/ml/models/whisper_models/tree/main' for available files."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, BarkModel, WhisperProcessor, WhisperForConditionalGeneration\n",
    "import sounddevice as sd\n",
    "import scipy.io.wavfile as wavfile\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import wave\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class VoiceAssistant:\n",
    "    def __init__(self):\n",
    "        print(\"Initializing voice assistant... This might take a minute...\")\n",
    "        \n",
    "        # Initialize Bark\n",
    "        self.bark_processor = AutoProcessor.from_pretrained(\"suno/bark\")\n",
    "        self.bark_model = BarkModel.from_pretrained(\"suno/bark\")\n",
    "        self.voice_preset = \"v2/en_speaker_6\"\n",
    "        \n",
    "        # Initialize Whisper\n",
    "        self.whisper_processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
    "        self.whisper_model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")\n",
    "        \n",
    "        # Set device\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.whisper_model.to(self.device)\n",
    "        self.bark_model.to(self.device)\n",
    "        \n",
    "        print(\"Initialization complete! Ready to chat!\")\n",
    "    \n",
    "    def speak(self, text):\n",
    "        \"\"\"Convert text to speech using Bark\"\"\"\n",
    "        inputs = self.bark_processor(text, voice_preset=self.voice_preset)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(self.device)\n",
    "            \n",
    "        audio_array = self.bark_model.generate(**inputs)\n",
    "        audio_array = audio_array.cpu().numpy().squeeze()\n",
    "        \n",
    "        # Normalize audio\n",
    "        audio_array = audio_array / np.max(np.abs(audio_array))\n",
    "        \n",
    "        # Play audio\n",
    "        sd.play(audio_array, samplerate=24000)\n",
    "        sd.wait()\n",
    "    \n",
    "    def listen(self, duration=5, sample_rate=16000):\n",
    "        \"\"\"Record audio and convert to text using Whisper\"\"\"\n",
    "        print(\"Listening...\")\n",
    "        \n",
    "        # Record audio\n",
    "        recording = sd.rec(int(duration * sample_rate), \n",
    "                         samplerate=sample_rate, \n",
    "                         channels=1)\n",
    "        sd.wait()\n",
    "        \n",
    "        # Save recording temporarily\n",
    "        wavfile.write(\"temp_recording.wav\", sample_rate, recording)\n",
    "        \n",
    "        # Load audio for Whisper\n",
    "        with wave.open(\"temp_recording.wav\", 'rb') as audio_file:\n",
    "            audio = audio_file.readframes(audio_file.getnframes())\n",
    "            audio_array = np.frombuffer(audio, dtype=np.int16).astype(np.float32) / 32768.0\n",
    "        \n",
    "        # Process with Whisper\n",
    "        input_features = self.whisper_processor(\n",
    "            audio_array, \n",
    "            sampling_rate=sample_rate, \n",
    "            return_tensors=\"pt\"\n",
    "        ).input_features.to(self.device)\n",
    "        \n",
    "        # Generate token ids\n",
    "        predicted_ids = self.whisper_model.generate(input_features)\n",
    "        \n",
    "        # Decode token ids to text\n",
    "        transcription = self.whisper_processor.batch_decode(\n",
    "            predicted_ids, \n",
    "            skip_special_tokens=True\n",
    "        )[0]\n",
    "        \n",
    "        return transcription.strip().lower()\n",
    "    \n",
    "    def chat(self):\n",
    "        \"\"\"Main conversation loop\"\"\"\n",
    "        self.speak(\"Hello! I'm your voice assistant. How can I help you today?\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                # Listen to user input\n",
    "                user_input = self.listen()\n",
    "                print(f\"You said: {user_input}\")\n",
    "                \n",
    "                # Check for exit commands\n",
    "                if user_input in ['goodbye', 'bye', 'exit', 'quit']:\n",
    "                    self.speak(\"Goodbye! Have a great day!\")\n",
    "                    break\n",
    "                \n",
    "                # Generate response (you can make this more sophisticated)\n",
    "                response = f\"You said: {user_input}\"\n",
    "                print(f\"Assistant: {response}\")\n",
    "                self.speak(response)\n",
    "                \n",
    "                clear_output(wait=True)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {str(e)}\")\n",
    "                break\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    assistant = VoiceAssistant()\n",
    "    assistant.chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "688ba2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to your fully local conversational assistant!\n",
      "Initializing...\n",
      "Loading models locally...\n",
      "Ready! Press Ctrl+C to exit.\n",
      "Listening... Speak now!\n",
      "Recording finished.\n",
      "Transcribing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hissain/myenv/lib/python3.11/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "No GPU being used. Careful, inference might be very slow!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User said: Hello. How are you?\n",
      "Generating response...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ecb126ab728497b8fa4b6ab9486bab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text_2.pt:   0%|          | 0.00/5.35G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exiting...\n",
      "Thank you for using the local voice assistant!\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "import whisper\n",
    "from bark import SAMPLE_RATE, generate_audio\n",
    "import numpy as np\n",
    "import wave\n",
    "import os\n",
    "from bark.generation import preload_models\n",
    "\n",
    "model_dir_whisper = os.path.expanduser(\"~/ml/models/whisper_models\")\n",
    "model_dir_bark = os.path.expanduser(\"~/ml/models/bark_models\")\n",
    "\n",
    "def load_models():\n",
    "    print(\"Loading models locally...\")\n",
    "    try:\n",
    "        whisper_model = whisper.load_model(\"small\", download_root=model_dir_whisper)\n",
    "        return whisper_model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading models: {e}\")\n",
    "        return None\n",
    "\n",
    "def record_audio(filename=\"input_audio.wav\", duration=5, samplerate=16000):\n",
    "    print(\"Listening... Speak now!\")\n",
    "    try:\n",
    "        # Ensure the audio data is float32 for better compatibility\n",
    "        audio_data = sd.rec(int(duration * samplerate), \n",
    "                          samplerate=samplerate, \n",
    "                          channels=1, \n",
    "                          dtype='float32')\n",
    "        sd.wait()\n",
    "        print(\"Recording finished.\")\n",
    "        \n",
    "        # Convert float32 to int16 for WAV file\n",
    "        audio_data_int = (audio_data * 32767).astype(np.int16)\n",
    "        write(filename, samplerate, audio_data_int)\n",
    "        return filename\n",
    "    except Exception as e:\n",
    "        print(f\"Error recording audio: {e}\")\n",
    "        return None\n",
    "\n",
    "def transcribe_audio(filename, whisper_model):\n",
    "    print(\"Transcribing...\")\n",
    "    try:\n",
    "        result = whisper_model.transcribe(filename)\n",
    "        return result[\"text\"].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error transcribing audio: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def speak_text(text, output_file=\"output_audio.wav\"):\n",
    "    print(\"Generating response...\")\n",
    "    try:\n",
    "        audio_array = generate_audio(text)\n",
    "        \n",
    "        with wave.open(output_file, \"wb\") as wf:\n",
    "            wf.setnchannels(1)  # Mono audio\n",
    "            wf.setsampwidth(2)  # 16-bit audio\n",
    "            wf.setframerate(SAMPLE_RATE)  # Use Bark's sample rate constant\n",
    "            audio_int16 = (audio_array * 32767).astype(np.int16)\n",
    "            wf.writeframes(audio_int16.tobytes())\n",
    "        \n",
    "        print(f\"Response saved as {output_file}\")\n",
    "        return output_file\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating speech: {e}\")\n",
    "        return None\n",
    "\n",
    "def play_audio(file_path):\n",
    "    \"\"\"Play audio file\"\"\"\n",
    "    try:\n",
    "        with wave.open(file_path, 'rb') as wf:\n",
    "            rate = wf.getframerate()\n",
    "            data = wf.readframes(wf.getnframes())\n",
    "            audio_data = np.frombuffer(data, dtype=np.int16)\n",
    "            sd.play(audio_data, samplerate=rate, blocking=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error playing audio: {e}\")\n",
    "\n",
    "def conversational_assistant():\n",
    "    print(\"Welcome to your fully local conversational assistant!\")\n",
    "    print(\"Initializing...\")\n",
    "    \n",
    "    os.makedirs(model_dir_whisper, exist_ok=True)\n",
    "    os.makedirs(model_dir_bark, exist_ok=True)\n",
    "    \n",
    "    # Load models\n",
    "    whisper_model = load_models()\n",
    "    if whisper_model is None:\n",
    "        print(\"Failed to load models. Exiting...\")\n",
    "        return\n",
    "    \n",
    "    print(\"Ready! Press Ctrl+C to exit.\")\n",
    "    try:\n",
    "        while True:\n",
    "            # Record user input\n",
    "            input_audio = record_audio(duration=5)\n",
    "            if input_audio is None:\n",
    "                continue\n",
    "            \n",
    "            # Transcribe audio\n",
    "            user_input = transcribe_audio(input_audio, whisper_model)\n",
    "            if not user_input:\n",
    "                continue\n",
    "            \n",
    "            print(f\"User said: {user_input}\")\n",
    "            \n",
    "            if user_input.lower() in [\"exit\", \"quit\", \"bye\", \"goodbye\"]:\n",
    "                print(\"Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            # Generate and speak response\n",
    "            response_text = f\"You said: {user_input}. How can I assist you further?\"\n",
    "            response_audio = speak_text(response_text)\n",
    "            if response_audio:\n",
    "                play_audio(response_audio)\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nExiting...\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        print(\"Thank you for using the local voice assistant!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    conversational_assistant()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
